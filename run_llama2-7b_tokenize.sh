
k=1
run_name="ft_Llama2-7b_tokenize"
lr=3e-4
bs=200
per_device_eval_batch_size=1
use_lora=0
epochs=0.01
gradient_checkpointing=True
gradient_accumulation_steps=1
lora_r=8
block_size=512
ds_config=configs/ds_config_zero2_custom_optimizer.json
# model_name_or_path=configs/llama_350m.json
# model_name_or_path=pinkmanlove/llama-7b-hf
model_name_or_path=meta-llama/Llama-2-7b-hf
# model_name_or_path=microsoft/phi-1_5
# model_name_or_path=gpt2
exp_name="${run_name}_lr${lr}";
# data_path="data/gpt4_v2"
data_path="data/c4_10G"
warmup_ratio=0.01
echo ${data_path}
eval_dataset_path="data/continue_half_news_wiki_formated_eval/"
test_dataset_path="data/continue_half_news_wiki_formated_test"
num_portions=${k}
selected_portion=1
optimizer_name=Adamw
freeze_layers="--freeze_layers  0 11 "
freeze_percentage=90
freeze_strategy="random"
local_seed=$RANDOM
bash ./scripts/run_finetune_relora.sh ${exp_name} ${data_path} ${lr} ${bs} ${model_name_or_path} ${use_lora} ${ds_config} ${epochs} ${gradient_checkpointing} ${gradient_accumulation_steps} ${lora_r} ${eval_dataset_path} ${block_size} ${per_device_eval_batch_size} ${warmup_ratio} ${num_portions} ${selected_portion} ${optimizer_name}  "--master_port=10003 --include localhost:0" 
